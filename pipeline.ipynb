{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M0G_RPyQ5sQ"
      },
      "source": [
        "# Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nir7iKeNMVNJ",
        "outputId": "c2d0a3f9-f8fa-488e-fa04-2e4945e06cc7"
      },
      "outputs": [],
      "source": [
        "%pip install pandas requests tqdm pyarrow seaborn matplotlib scikit-learn-intelex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNtMmor4jI5W"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from typing import Literal\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import pyarrow as pa\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MnpO3vgr5Uo"
      },
      "outputs": [],
      "source": [
        "# please add your API keys\n",
        "HUGGINGFACE_API_KEY = \"\"\n",
        "MISTRAL_API_KEY = \"\"\n",
        "OPENAI_API_KEY = \"\"\n",
        "DEEPINFRA_API_KEY = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeOOxF6Hr5Uq"
      },
      "source": [
        "### data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOi7Tudo6REA"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "def read_csv_alt_dict(filename: str, main_column: str, columns_to_join: list[str], joined_column_name: str, remove_null_rows: Optional[bool] = False) -> dict[str, list[str]]:\n",
        "    # read csv file, splitting the strings at commas and making a list\n",
        "    df = pd.read_csv(filename, converters={col: lambda x: x.split(',') for col in columns_to_join})\n",
        "    # joining the lists in the columns into one\n",
        "    df[joined_column_name] = df[columns_to_join].apply(lambda row: sum(row, []), axis=1)\n",
        "    # only keep those columns\n",
        "    df = df[[main_column, joined_column_name]]\n",
        "    # only keep items in the list that are not 'NULL'\n",
        "    df[joined_column_name] = df[joined_column_name].apply(lambda x: [item for item in x if item != 'NULL'])\n",
        "    # remove any empty rows if so desired\n",
        "    if remove_null_rows:\n",
        "        df = df[df[joined_column_name].apply(bool)]\n",
        "\n",
        "    # Create a dictionary of names and alternative names\n",
        "    name_dict = defaultdict(list)\n",
        "    for _, row in df.iterrows():\n",
        "        name_dict[row[main_column]].append(row[main_column])  # Add the main name itself\n",
        "        if isinstance(row[joined_column_name], list):\n",
        "            for alt in row[joined_column_name]:\n",
        "                if alt != 'None':\n",
        "                    name_dict[row[main_column]].append(alt)\n",
        "\n",
        "    return dict(name_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aX3h9sAr5Ut"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(input_sentences: list[str]) -> list[str]:\n",
        "    classes_semantically_equal = {\n",
        "        \"holding\": [\"holding\", \"brandishing\", \"carrying\", \"playing\", \"cradling\", \"supporting\", \"pouring\", \"drawing\",\n",
        "                    \"picking\", \"touching\", \"containing\", \"carrying\", \"drawing_out\", \"raising\", \"removing\", \"collecting\",\n",
        "                    \"hanging_on\", \"shouldering\"],\n",
        "        \"wearing\": [\"wearing\", \"covered_with\"],\n",
        "        \"resting_on\": [\"resting_on\", \"reclining_on\", \"leaning_on\", \"setting_on\", \"set_on\"],\n",
        "        \"seated_on\": [\"sitting_on\", \"sitting\", \"seated_in\", \"seated_on\", \"riding\", \"riding_on\"],\n",
        "        \"grasping\": [\"grasping\", \"scooping\", \"reach_out\", \"plucking\", \"clasping\", \"strangling\", \"placing\"],\n",
        "        \"standing\": [\"standing\", \"standing_on\", \"standing_in\", \"driving\"],\n",
        "    }\n",
        "\n",
        "    persons_semantically_equal = read_csv_alt_dict(\"nlp_list_person.csv\", \"name\", [\"alternativenames\", \"typos\"], \"alt_names\", True)\n",
        "\n",
        "    # combine both dicts\n",
        "    semantically_equal_words: dict[str, list[str]] = dict(**classes_semantically_equal, **persons_semantically_equal)\n",
        "\n",
        "    processed_sentences = []\n",
        "    for sentence in input_sentences:\n",
        "        for main_word, alt_word_list in semantically_equal_words.items():\n",
        "            for alt_word in alt_word_list:\n",
        "                # replace all alt_words with main_word\n",
        "                sentence = re.sub(r'\\b' + re.escape(alt_word) + r'\\b', main_word, sentence, flags=re.IGNORECASE)\n",
        "        processed_sentences.append(sentence)\n",
        "\n",
        "    return processed_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao2jz_V5r5Uv"
      },
      "source": [
        "### functions for embedding retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br0Tm7pJr5Uw"
      },
      "outputs": [],
      "source": [
        "# embeddings for hugging face inference API\n",
        "def get_hf_embeddings(input_sentences: list[str], model_id: str, api_key: str) -> pd.DataFrame:\n",
        "    # post request to hf\n",
        "    def get_embeddings_batch(sentence_batch: list[str]) -> pd.DataFrame:\n",
        "        response: list[list[float]] = requests.post(\n",
        "            api_url,\n",
        "            headers=headers,\n",
        "            json={\n",
        "                \"inputs\": sentence_batch,\n",
        "                \"options\": {\"wait_for_model\": True}\n",
        "            }\n",
        "        ).json()\n",
        "        df = pd.DataFrame({\n",
        "            'sentence': sentence_batch,\n",
        "            'embedding': response\n",
        "        })\n",
        "        return df\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "    api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "    processed_sentences = preprocess_sentence(input_sentences)\n",
        "\n",
        "    all_embeddings = pd.DataFrame()\n",
        "    for i in tqdm(range(0, len(processed_sentences), BATCH_SIZE), miniters=500, disable=True):\n",
        "        batch = processed_sentences[i:i + BATCH_SIZE]\n",
        "        embeddings_batch = get_embeddings_batch(batch)\n",
        "        all_embeddings = pd.concat([all_embeddings, embeddings_batch], ignore_index=True)\n",
        "\n",
        "    return all_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOg3OMoFr5Ux"
      },
      "outputs": [],
      "source": [
        "# embeddings for mistral and openAI\n",
        "def get_embeddings_universal(input_sentences: list[str], model_id: str, api_url: str, api_key: str) -> pd.DataFrame:\n",
        "    # post request\n",
        "    def get_embeddings_batch(sentence_batch: list[str]) -> pd.DataFrame:\n",
        "        response = requests.post(\n",
        "            api_url,\n",
        "            headers=headers,\n",
        "            json = {\n",
        "                \"model\": model_id,\n",
        "                \"input\": sentence_batch,\n",
        "                \"encoding_format\": \"float\"\n",
        "            }\n",
        "        ).json()\n",
        "        data = response.get(\"data\") or []\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            'sentence': sentence_batch,\n",
        "            'embedding': [item.get(\"embedding\", 0) for item in data]\n",
        "        })\n",
        "        return df\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "    processed_sentences = preprocess_sentence(input_sentences)\n",
        "\n",
        "    all_embeddings = pd.DataFrame()\n",
        "    for i in tqdm(range(0, len(processed_sentences), BATCH_SIZE), miniters=500, disable=True):\n",
        "        batch = processed_sentences[i:i + BATCH_SIZE]\n",
        "        embeddings_batch = get_embeddings_batch(batch)\n",
        "        all_embeddings = pd.concat([all_embeddings, embeddings_batch], ignore_index=True)\n",
        "\n",
        "    return all_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLvtqJWwr5U3"
      },
      "source": [
        "### methods for calculating embeddings using multiple models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLEKTujVFgrL"
      },
      "outputs": [],
      "source": [
        "def calculate_embeddings(sentences: list[str],\n",
        "                         model: Literal[\n",
        "                             \"openai_3_small\",\n",
        "                             \"openai_3_large\",\n",
        "                             \"minilm-l12-v2\",\n",
        "                             \"mpnet_base_v2\",\n",
        "                             \"mistral-embed\",\n",
        "                             \"baai_bge_large_v1.5\",\n",
        "                             \"embedding_e5_large_v2\"\n",
        "                            ]\n",
        "                        ) -> pd.DataFrame:\n",
        "    match model:\n",
        "        case \"openai_3_small\":\n",
        "            model_id = \"text-embedding-3-small\"\n",
        "            api_url = \"https://api.openai.com/v1/embeddings\"\n",
        "            openai_small_embeddings = get_embeddings_universal(sentences, model_id, api_url, OPENAI_API_KEY)\n",
        "            return openai_small_embeddings\n",
        "        case \"openai_3_large\":\n",
        "            model_id = \"text-embedding-3-large\"\n",
        "            api_url = \"https://api.openai.com/v1/embeddings\"\n",
        "            openai_large_embeddings = get_embeddings_universal(sentences, model_id, api_url, OPENAI_API_KEY)\n",
        "            return openai_large_embeddings\n",
        "        case \"minilm-l12-v2\":\n",
        "            model_id = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
        "            miniLM_embeddings = get_hf_embeddings(sentences, model_id, HUGGINGFACE_API_KEY)\n",
        "            return miniLM_embeddings\n",
        "        case \"mpnet_base_v2\":\n",
        "            model_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "            mpnet_base_v2_embeddings = get_hf_embeddings(sentences, model_id, HUGGINGFACE_API_KEY)\n",
        "            return mpnet_base_v2_embeddings\n",
        "        case \"mistral-embed\":\n",
        "            model_id = \"mistral-embed\"\n",
        "            api_url = \"https://api.mistral.ai/v1/embeddings\"\n",
        "            mistral_embeddings = get_embeddings_universal(sentences, model_id, api_url, MISTRAL_API_KEY)\n",
        "            return mistral_embeddings\n",
        "        case \"baai_bge_large_v1.5\":\n",
        "            model_id = \"BAAI/bge-large-en-v1.5\"\n",
        "            api_url = \"https://api.deepinfra.com/v1/openai/embeddings\"\n",
        "            bge_large_embeddings = get_embeddings_universal(sentences, model_id, api_url, DEEPINFRA_API_KEY)\n",
        "            return bge_large_embeddings\n",
        "        case \"embedding_e5_large_v2\":\n",
        "            model_id = \"intfloat/e5-large-v2\"\n",
        "            api_url = \"https://api.deepinfra.com/v1/openai/embeddings\"\n",
        "            e5_large_embeddings = get_embeddings_universal(sentences, model_id, api_url, DEEPINFRA_API_KEY)\n",
        "            return e5_large_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "iri_Q_Ym5PaA",
        "outputId": "1d35f09d-9b7c-4773-87e4-8482298e46f6"
      },
      "outputs": [],
      "source": [
        "# test a specific model\n",
        "e5_large_v2_result = calculate_embeddings(coin_desc['sentence'].iloc[0:1].to_list(), \"embedding_e5_large_v2\")\n",
        "\n",
        "print(e5_large_v2_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoTeUkvnr5U4"
      },
      "outputs": [],
      "source": [
        "from itertools import batched\n",
        "\n",
        "def query_all_models(sentences: list[str]) -> pd.DataFrame:\n",
        "    models = [\n",
        "        \"openai_3_small\",\n",
        "        \"openai_3_large\",\n",
        "        \"minilm-l12-v2\",\n",
        "        \"mpnet_base_v2\",\n",
        "        \"mistral-embed\",\n",
        "        \"baai_bge_large_v1.5\",\n",
        "        \"embedding_e5_large_v2\"\n",
        "    ]\n",
        "\n",
        "    BATCH_SIZE = 256\n",
        "\n",
        "    combined_df = pd.DataFrame({'sentence': sentences})\n",
        "    batch_number = 1\n",
        "\n",
        "    for sentence_batch in batched(sentences, BATCH_SIZE):\n",
        "        print(f\"processed batch {batch_number} out of {len(sentences)//BATCH_SIZE}\")\n",
        "        with ThreadPoolExecutor(max_workers=len(models)) as executor:\n",
        "            # create dictionary with model and pd.DataFrame\n",
        "            futures = {model: executor.submit(calculate_embeddings, sentence_batch, model) for model in models}\n",
        "            # wait for the actual results\n",
        "            results = {model: future.result() for model, future in futures.items()}\n",
        "\n",
        "            # append to combined_df\n",
        "            for model, df in results.items():\n",
        "                combined_df[f\"embedding_{model}\"] = combined_df[f\"embedding_{model}\"].append(df['embedding']).reset_index(drop=True)\n",
        "\n",
        "    return combined_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3sfz_S0r5Uz"
      },
      "source": [
        "### Import description sentences for each coin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGOzR8V2PQHv"
      },
      "outputs": [],
      "source": [
        "# Einlesen der descriptions\n",
        "coin_desc = pd.read_csv(\"data_descriptions_export.csv\")\n",
        "coin_desc.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "# coin_desc = coin_desc.drop('Unnamed: 0.1', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "acGhtYC-URBv",
        "outputId": "bb3f54f1-d4db-4143-f163-86f755b8f0db"
      },
      "outputs": [],
      "source": [
        "coin_desc.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSqg26LVblRm"
      },
      "source": [
        "### calculate the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYT8Vj-Kbqnu"
      },
      "outputs": [],
      "source": [
        "embeddings = query_all_models(coin_desc.to_list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUUrmnU2ceMW"
      },
      "outputs": [],
      "source": [
        "embeddings.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_86zZAr5U7"
      },
      "source": [
        "### export or import calculated embeddings to feather file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bqQ4zoZr5U7"
      },
      "outputs": [],
      "source": [
        "import pyarrow.feather as feather\n",
        "\n",
        "feather.write_feather(embeddings, 'embeddings.feather')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JJNsk2qDThj"
      },
      "outputs": [],
      "source": [
        "embeddings = feather.read_feather('embeddings.feather')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7OTQwxjr5U8"
      },
      "source": [
        "### data evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC6frOHar5U9"
      },
      "outputs": [],
      "source": [
        "def get_top_similar_coins(query_strings: list[str], embeddings_df: pd.DataFrame, n=5) -> dict:\n",
        "    # list of models\n",
        "    models = [\n",
        "        \"openai_3_small\",\n",
        "        \"openai_3_large\",\n",
        "        \"minilm-l12-v2\",\n",
        "        \"mpnet_base_v2\",\n",
        "        \"mistral-embed\",\n",
        "        \"baai_bge_large_v1.5\",\n",
        "        \"embedding_e5_large_v2\"\n",
        "    ]\n",
        "\n",
        "    query_embeddings = query_all_models(query_strings)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for model in models:\n",
        "        # extract embeddings for current model\n",
        "        db_embeddings = np.stack(embeddings_df[f'embedding_{model}'].values)\n",
        "        query_model_embeddings = np.stack(query_embeddings[f'embedding_{model}'].values)\n",
        "\n",
        "        similarities = cosine_similarity(query_model_embeddings, db_embeddings)\n",
        "\n",
        "        # find n nearest neighbors\n",
        "        model_results = []\n",
        "        for i, similarity_scores in enumerate(similarities):\n",
        "            top_indices = np.argsort(similarity_scores)[-n:][::-1]\n",
        "            top_similarities = similarity_scores[top_indices]\n",
        "\n",
        "            query_results = []\n",
        "            for idx, sim in zip(top_indices, top_similarities):\n",
        "                query_results.append({\n",
        "                    'query': query_strings[i],\n",
        "                    'similar_coin': embeddings_df.iloc[idx]['sentence'],\n",
        "                    'similarity_score': sim\n",
        "                })\n",
        "            model_results.append(query_results)\n",
        "\n",
        "        results[model] = model_results\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "u_been1-PDcN",
        "outputId": "f558418a-e70e-4232-b26b-0aaaade12fc6"
      },
      "outputs": [],
      "source": [
        "def results_to_dataframe(results: dict):\n",
        "    data = []\n",
        "    for model, model_results in results.items():\n",
        "        for query_results in model_results:\n",
        "            for rank, result in enumerate(query_results, 1):\n",
        "                data.append({\n",
        "                    'Model': model,\n",
        "                    'Query': result['query'],\n",
        "                    'Rank': rank,\n",
        "                    'Similar Coin': result['similar_coin'],\n",
        "                    'Similarity Score': result['similarity_score']\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df = df.sort_values(['Model', 'Query', 'Rank'])\n",
        "    df['Similarity Score'] = df['Similarity Score'].apply(lambda x: f\"{x:.4f}\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie_rDnHben8R"
      },
      "outputs": [],
      "source": [
        "# example usage\n",
        "query = [\"Emperor holding weapon\"]\n",
        "similar_coins_dict = get_top_similar_coins(query, embeddings, n=3)\n",
        "similar_coins_df = results_to_dataframe(similar_coins_dict)\n",
        "\n",
        "similar_coins_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goK-F4G4hzVH"
      },
      "source": [
        "# evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ4B93GB4z4s"
      },
      "source": [
        "Evaluierung für den zuletzt verwendeten Prompt. (df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I8anQTBZhzCm",
        "outputId": "e7a4b89f-5b34-4e7c-ab5a-c5a6b1f05763"
      },
      "outputs": [],
      "source": [
        "# 1. Häufigkeit der Ergebnisse\n",
        "ergebnis_haeufigkeit = df['Similar Coin'].value_counts()\n",
        "\n",
        "print(\"Häufigkeit der Ergebnisse:\")\n",
        "print(ergebnis_haeufigkeit)\n",
        "\n",
        "# 2. Identifizierung seltener Ergebnisse (z.B. weniger als 5% der häufigsten Antwort)\n",
        "schwellenwert = ergebnis_haeufigkeit.max() * 0.3\n",
        "seltene_ergebnisse = ergebnis_haeufigkeit[ergebnis_haeufigkeit <= schwellenwert]\n",
        "\n",
        "print(\"\\nSeltene Ergebnisse:\")\n",
        "print(seltene_ergebnisse)\n",
        "\n",
        "# 3. Analyse der Modelle, die seltene Antworten liefern\n",
        "seltene_antworten_modelle = df[df['Similar Coin'].isin(seltene_ergebnisse.index)].groupby('Model')['Similar Coin'].count()\n",
        "\n",
        "print(\"\\nModelle mit seltenen Antworten:\")\n",
        "print(seltene_antworten_modelle)\n",
        "\n",
        "# 4. Visualisierung\n",
        "plt.figure(figsize=(12, 6))\n",
        "ergebnis_haeufigkeit.plot(kind='bar')\n",
        "plt.title('Häufigkeit der Ergebnisse')\n",
        "plt.xlabel('Ergebnis')\n",
        "plt.ylabel('Anzahl')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "seltene_antworten_modelle.plot(kind='bar')\n",
        "plt.title('Modelle mit seltenen Antworten')\n",
        "plt.xlabel('Modell')\n",
        "plt.ylabel('Anzahl seltener Antworten')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R97XjFQJ45Nr"
      },
      "source": [
        "Evaluierung für alle getätigten Prompts (combined_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "Tz-vQ3DA4EVl",
        "outputId": "d27f7ada-b7b9-4a12-bdc8-7e92e30f7f28"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Beispiel-Daten erstellen (entfernen, wenn tatsächliche Daten bereits vorliegen)\n",
        "# df = pd.DataFrame({\n",
        "#     'Query': ['Query1', 'Query1', 'Query2', 'Query2', 'Query3', 'Query3', 'Query1', 'Query2', 'Query3'],\n",
        "#     'Model': ['Model1', 'Model2', 'Model1', 'Model3', 'Model2', 'Model1', 'Model1', 'Model2', 'Model3'],\n",
        "#     'Similar Coin': ['CoinA', 'CoinB', 'CoinA', 'CoinC', 'CoinB', 'CoinD', 'CoinA', 'CoinB', 'CoinC']\n",
        "# })\n",
        "\n",
        "# Dictionary, um die seltenen Ergebnisse pro Modell für alle Queries zu speichern\n",
        "gesamt_seltene_ergebnisse_pro_modell = {}\n",
        "\n",
        "# Für jede eindeutige Query die Analyse durchführen\n",
        "for query in df['Query'].unique():\n",
        "    query_df = df[df['Query'] == query]\n",
        "\n",
        "    # 1. Häufigkeit der Ergebnisse für diese Query\n",
        "    ergebnis_haeufigkeit = query_df['Similar Coin'].value_counts()\n",
        "\n",
        "    # 2. Identifizierung seltener Ergebnisse (z.B. weniger als 5% der häufigsten Antwort)\n",
        "    schwellenwert = ergebnis_haeufigkeit.max() * 0.3\n",
        "    seltene_ergebnisse = ergebnis_haeufigkeit[ergebnis_haeufigkeit <= schwellenwert]\n",
        "\n",
        "    # 3. Bestimmen, welche Modelle die seltenen Ergebnisse geliefert haben\n",
        "    seltene_ergebnisse_df = query_df[query_df['Similar Coin'].isin(seltene_ergebnisse.index)]\n",
        "    seltene_ergebnisse_pro_modell = seltene_ergebnisse_df['Model'].value_counts()\n",
        "\n",
        "    # Hinzufügen der seltenen Ergebnisse für diese Query zur Gesamtliste\n",
        "    for model, count in seltene_ergebnisse_pro_modell.items():\n",
        "        if model in gesamt_seltene_ergebnisse_pro_modell:\n",
        "            gesamt_seltene_ergebnisse_pro_modell[model] += count\n",
        "        else:\n",
        "            gesamt_seltene_ergebnisse_pro_modell[model] = count\n",
        "\n",
        "# Umwandlung des Gesamt-Dictionary in eine Series für die Visualisierung\n",
        "gesamt_seltene_ergebnisse_pro_modell = pd.Series(gesamt_seltene_ergebnisse_pro_modell)\n",
        "\n",
        "# Gesamtauswertung der Modelle\n",
        "print(\"\\nGesamte Anzahl der seltenen Ergebnisse pro Modell:\")\n",
        "print(gesamt_seltene_ergebnisse_pro_modell)\n",
        "\n",
        "# Visualisierung der seltenen Ergebnisse pro Modell für alle Queries\n",
        "plt.figure(figsize=(12, 6))\n",
        "gesamt_seltene_ergebnisse_pro_modell.plot(kind='bar', color='orange')\n",
        "plt.title('Gesamte Anzahl der seltenen Ergebnisse pro Modell für alle Queries')\n",
        "plt.xlabel('Modell')\n",
        "plt.ylabel('Anzahl der seltenen Ergebnisse')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssj0Hxh7bB7A"
      },
      "source": [
        "# Visualizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1lAD_M1rKaX",
        "outputId": "6b16fa4a-bfc9-4c61-eeb2-3707268c07ce"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def get_top_n_things(things_dict: dict[str, list[str]], search_body: pd.DataFrame, n: int) -> list[str]:\n",
        "    # Initialize a count dictionary\n",
        "    count_dict = defaultdict(int)\n",
        "\n",
        "    # Search for occurrences in the sentences\n",
        "    for sentence in search_body['sentence']:\n",
        "        for person, names in things_dict.items():\n",
        "            sentence_words = sentence.split()\n",
        "            if any(name in sentence_words for name in names):\n",
        "                count_dict[person] += 1\n",
        "\n",
        "    # Convert the count dictionary to a sorted list\n",
        "    sorted_things = sorted(count_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    return [item[0] for item in sorted_things[:n]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M28Rg_H-0mS"
      },
      "outputs": [],
      "source": [
        "def generate_grouping_row(things_to_highlight: list[str], alt_words_dict: dict[str, list[str]], search_body: pd.DataFrame, grouping_column_name: str) -> None:\n",
        "\n",
        "    search_body[grouping_column_name] = 'other'\n",
        "    # Update the person_grouping column based on occurrences in the sentences\n",
        "    for index, row in search_body.iterrows():\n",
        "        sentence = row['sentence']\n",
        "        for main_word, alt_word_list in alt_words_dict.items():\n",
        "            sentence_words = sentence.split()\n",
        "            if any(alt_word in sentence_words for alt_word in alt_word_list) and main_word in things_to_highlight:\n",
        "                # print([main_word, sentence])\n",
        "                embeddings.at[index, grouping_column_name] = main_word\n",
        "                break\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlTTLUE6-6iO"
      },
      "outputs": [],
      "source": [
        "from matplotlib.axes import Axes\n",
        "import numpy as np\n",
        "# patch scikit-learn with intel's extension. Also compatible with AMD!\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_scatter_plot(search_body: pd.DataFrame, alt_words_dict: dict[str: list[str]], number_of_highlights: int, category: str, model: str) -> Axes:\n",
        "\n",
        "    top_things = get_top_n_things(alt_words_dict, search_body, number_of_highlights)\n",
        "\n",
        "    generate_grouping_row(top_things, alt_words_dict, embeddings, category)\n",
        "\n",
        "    top_things.sort(key=lambda s: s.casefold())\n",
        "    top_things.append('other')\n",
        "\n",
        "    # Set the figure size\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    default_palette = sns.color_palette(\"husl\", n_colors=(len(top_things)))\n",
        "    palette_colors = {k: 'gray' if k == \"other\" else default_palette[i] for i,k in enumerate(top_things)}\n",
        "\n",
        "\n",
        "    # Create the t-SNE transformation\n",
        "    tsne = TSNE(n_components=2, random_state=0).fit_transform(np.array(embeddings[model].to_list()))\n",
        "\n",
        "    # Create a scatter plot\n",
        "    ax = sns.scatterplot(x=tsne[:, 0], y=tsne[:, 1], alpha=1, hue=np.array(embeddings[category].to_list()), palette=palette_colors, hue_order=top_things, s=20)\n",
        "\n",
        "    plt.legend(markerscale=2)\n",
        "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
        "\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZsFt04egrms"
      },
      "outputs": [],
      "source": [
        "# dictionaries with important persons, objects, plants etc and their alternative names\n",
        "persons_dict = read_csv_alt_dict('nlp_list_person.csv', 'name', ['alternativenames', 'typos'], 'alt_names')\n",
        "objects_dict = read_csv_alt_dict('nlp_list_obj.csv', 'name_en', ['alternativenames_en', 'typos_en'], 'alt_names')\n",
        "plants_dict = read_csv_alt_dict('nlp_list_plant.csv', 'name_en', ['alternativenames_en', 'typos_en'], 'alt_names')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxjAFyl0hF9I"
      },
      "outputs": [],
      "source": [
        "# make plots for the embeddings colors based on persons, objects and so on\n",
        "\n",
        "import matplotlib.ticker as ticker\n",
        "models = [\n",
        "    \"openai_3_small\",\n",
        "    \"openai_3_large\",\n",
        "    \"minilm-l12-v2\",\n",
        "    \"mpnet_base_v2\",\n",
        "    \"mistral-embed\",\n",
        "    \"baai_bge_large_v1.5\",\n",
        "    \"embedding_e5_large_v2\"\n",
        "]\n",
        "\n",
        "for thing_name, things_dict in [(\"persons\", persons_dict), (\"objects\", objects_dict), (\"plants\", plants_dict)]:\n",
        "    for model in models:\n",
        "        ax = make_scatter_plot(embeddings, things_dict, 25, f\"{thing_name}_grouping\", f\"embedding_{model}\")\n",
        "        ax.xaxis.set_major_locator(ticker.NullLocator())\n",
        "        ax.yaxis.set_major_locator(ticker.NullLocator())\n",
        "        plt.savefig(f\"{thing_name}_{model}_scatterplot.png\")\n",
        "        print(f\"saved plot for {thing_name}_{model}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
